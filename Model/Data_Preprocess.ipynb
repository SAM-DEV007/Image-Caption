{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33652e3-5a03-4659-a262-83196a614d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad9eac2-2482-4b8a-a047-5327cf7af825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flickr8k():\n",
    "    path = pathlib.Path.cwd()\n",
    "    \n",
    "    if len(list(path.rglob('*'))) < 16197:\n",
    "        tf.keras.utils.get_file(\n",
    "            origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "            cache_dir=path,\n",
    "            cache_subdir='datasets',\n",
    "            extract=True)\n",
    "        tf.keras.utils.get_file(\n",
    "            origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "            cache_dir=path,\n",
    "            cache_subdir='datasets',\n",
    "            extract=True)\n",
    "\n",
    "    path = pathlib.Path(str(path) + '\\\\datasets')\n",
    "    \n",
    "    captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "    captions = (line.split('\\t') for line in captions)\n",
    "    captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "    \n",
    "    cap_dict = collections.defaultdict(list)\n",
    "    for fname, cap in captions:\n",
    "        cap_dict[fname].append(cap)\n",
    "    \n",
    "    train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "    train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "    \n",
    "    test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "    test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "    \n",
    "    train_ds = tf.data.experimental.from_list(train_captions)\n",
    "    test_ds = tf.data.experimental.from_list(test_captions)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02140ec0-653a-44c0-9eaa-012628c91b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, test_raw = flickr8k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd09b85-adc3-4809-892e-af03c085ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'D:\\\\Projects\\\\Image_Caption\\\\Image-Caption\\\\Model\\\\datasets\\\\Flicker8k_Dataset\\\\2513260012_03d33305cf.jpg', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'A black dog is running after a white dog in the snow .'\n",
      " b'Black dog chasing brown dog through snow'\n",
      " b'Two dogs chase each other across the snowy ground .'\n",
      " b'Two dogs play together in the snow .'\n",
      " b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for ex_path, ex_captions in train_raw.take(1):\n",
    "    print(ex_path)\n",
    "    print(ex_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547b90f2-b4a5-47a3-be23-32a003a0077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE=(224, 224, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Large(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=True)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990abe13-62d9-4775-842b-cbf62f31b122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/Projects/Image_Caption/Image-Caption/Model/Model_Data/mobilenet_v3_large_weights.h5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(pathlib.Path.cwd() / 'Model_Data'):\n",
    "    os.mkdir(pathlib.Path.cwd() / 'Model_Data')\n",
    "shutil.move(pathlib.Path.home() / '.keras/models/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5', pathlib.Path.cwd() / 'Model_Data/mobilenet_v3_large_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee5e2a9-2f32-4034-9692-f02e7515218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea43d1e4-914a-4062-b50b-bab43b8ea739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(1, 7, 7, 960)\n"
     ]
    }
   ],
   "source": [
    "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
    "\n",
    "print(test_img_batch.shape)\n",
    "print(mobilenet(test_img_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c7ff2a-35e5-41e4-bb40-0737f9c1ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "    s = tf.strings.lower(s)\n",
    "    s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "    s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9fc936-60b8-4378-bba0-7b0bb55ffa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee9b92e-8973-44a7-8e0d-f0ca43e2b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f64676-935a-4209-8197-c3346dbb28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_shapes(images, captions):\n",
    "    caption_shape = einops.parse_shape(captions, 'b c')\n",
    "    captions = einops.rearrange(captions, 'b c -> (b c)')\n",
    "    images = einops.repeat(\n",
    "      images, 'b ... -> (b c) ...',\n",
    "      c = caption_shape['c'])\n",
    "    return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66176f94-b7cd-4485-8228-e46f102efc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "    tokens = tokenizer(txts)\n",
    "    \n",
    "    input_tokens = tokens[..., :-1]\n",
    "    label_tokens = tokens[..., 1:]\n",
    "    return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18105ce-9e73-435d-9ee2-28877b067951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "    ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "    \n",
    "    def to_tensor(inputs, labels):\n",
    "        (images, in_tok), out_tok = inputs, labels\n",
    "        return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "    \n",
    "    return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860615a1-0e98-4dc2-901a-0980d81ba658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_raw, tokenizer)\n",
    "test_ds = prepare_dataset(test_raw, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8f9fd4-06dd-43d2-8218-d0ebb0a61f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching\n",
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "    # Load the images and make batches.\n",
    "    ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "    \n",
    "    # Run the feature extractor on each batch\n",
    "    # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "    def gen():\n",
    "        for (images, captions) in tqdm.tqdm(ds): \n",
    "            feature_maps = image_model(images)\n",
    "            \n",
    "            feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "            yield feature_maps, captions\n",
    "    \n",
    "    # Wrap the generator in a new tf.data.Dataset.\n",
    "    new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "    \n",
    "    # Apply the tokenization \n",
    "    new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "    \n",
    "    # Save the dataset into shard files.\n",
    "    def shard_func(i, item):\n",
    "        return i % shards\n",
    "    new_ds.enumerate().save(save_path, shard_func=shard_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "527637f8-f2eb-4a32-9190-af4aed616d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [00:46,  4.00it/s]\n",
      "32it [00:07,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_raw, str(pathlib.Path.cwd() / 'Model_Data/train_cache'), mobilenet, tokenizer)\n",
    "save_dataset(test_raw, str(pathlib.Path.cwd() / 'Model_Data/test_cache'), mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f83f84a-1b79-4caf-947f-97df9107fa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 1, 4], dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1774ef6c-d8b3-4eed-9f6a-0c61d8d4ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'config': tokenizer.get_config(),\n",
    "             'weights': tokenizer.get_weights()}\n",
    "            , open(str(pathlib.Path.cwd() / 'Model_Data/tokenizer.pkl'), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
