{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d204fe-427c-40ef-9554-d2ac02876801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad13b091-8689-47d0-8c8f-a1931c45b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "    def custom_reader_func(datasets):\n",
    "        datasets = datasets.shuffle(1000)\n",
    "        return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "\n",
    "    ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "    \n",
    "    def drop_index(i, x):\n",
    "        return x\n",
    "    \n",
    "    ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b5d20a-5890-471e-96e6-3a0af4c398d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(str(pathlib.Path.cwd() / 'Model_Data/train_cache'))\n",
    "test_ds = load_dataset(str(pathlib.Path.cwd() / 'Model_Data/test_cache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb7b338-acde-40c5-8a1a-8d8f3b7b4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, max_length, depth):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "        \n",
    "        self.token_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=depth,\n",
    "            mask_zero=True)\n",
    "        \n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    def call(self, seq):\n",
    "        seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "        \n",
    "        x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "        x = x[tf.newaxis, :]  # (1, seq)\n",
    "        x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "        \n",
    "        return self.add([seq, x])\n",
    "\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn = self.mha(query=x, value=x,\n",
    "                        use_causal_mask=True)\n",
    "        x = self.add([x, attn])\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, y, **kwargs):\n",
    "        attn, attention_scores = self.mha(\n",
    "                 query=x, value=y,\n",
    "                 return_attention_scores=True)\n",
    "        \n",
    "        self.last_attention_scores = attention_scores\n",
    "        \n",
    "        x = self.add([x, attn])\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=units),\n",
    "            tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                                  key_dim=units,\n",
    "                                                  dropout=dropout_rate)\n",
    "        self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "        self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        in_seq, out_seq = inputs\n",
    "        \n",
    "        # Text input\n",
    "        out_seq = self.self_attention(out_seq)\n",
    "        out_seq = self.cross_attention(out_seq, in_seq)\n",
    "        self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "        out_seq = self.ff(out_seq)\n",
    "        return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e75ab89-f452-4d82-924d-e27cd78a9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=tokenizer.vocabulary_size(), **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.banned_tokens = banned_tokens\n",
    "        self.bias = None\n",
    "    \n",
    "    def adapt(self, ds):\n",
    "        counts = collections.Counter()\n",
    "        vocab_dict = {name: id \n",
    "                      for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "    \n",
    "        for tokens in tqdm.tqdm(ds):\n",
    "            counts.update(tokens.numpy().flatten())\n",
    "        \n",
    "        counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "        counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "        \n",
    "        counts_arr = counts_arr[:]\n",
    "        for token in self.banned_tokens:\n",
    "            counts_arr[vocab_dict[token]] = 0\n",
    "        \n",
    "        total = counts_arr.sum()\n",
    "        p = counts_arr/total\n",
    "        p[counts_arr==0] = 1.0\n",
    "        log_p = np.log(p)  # log(1) == 0\n",
    "        \n",
    "        entropy = -(log_p*p).sum()\n",
    "        \n",
    "        print()\n",
    "        print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "        print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "        \n",
    "        self.bias = log_p\n",
    "        self.bias[counts_arr==0] = -1e9\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95fb248-badb-47d1-9625-0e1209bd8bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m TokenOutput(\u001b[43mtokenizer\u001b[49m, banned_tokens\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m output_layer\u001b[38;5;241m.\u001b[39madapt(train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m inputs, labels: labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bee6c-97cf-47f4-b145-4e603352b846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
