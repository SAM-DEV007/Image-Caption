# Image-Caption
Generates a caption for the image.

Implementation of the tutorial project provided by [Tensorflow](https://www.tensorflow.org/text/tutorials/image_captioning). 

The aim of the project is to get an idea of how the captioning of an image occurs. It can, then, be modified to detect certain features from the image and generate customizable captions. These captions can also be integrated in social media apps like Instagram to generate captions for the post according to the image. It can also take account of the user behaviour and can customize the generated text.

It can create opportunities for customizable generation of captions from images without losing the creative touch. It can elaborate the features detected from the image. The hidden features detected can also act as a security measure by specifying the feature that is being detected as a calamity. For example, a person with a gun partially visible from the jacket. The feature extractor can detect the harmful object, here it is gun, and notify the officials describing the situation of weapon detection, its location and the description of the person and place.

# Requirements
- `pip install -r requirements.txt`.
- Python 3.x or greator. (Python 3.9 used for development here)
- To use the updated model:
    - Download the model from [here](https://drive.google.com/file/d/1cYcKaNmyPMS0Zq-x4ZmVGq6JhIH4fLQe/view?usp=sharing).
    - Copy and extract it to `Model/Model_Data/`, replace the folder if necessary.
    - Make sure the name of the folder is `weights`.

# Installation
## Clone the repository
```bash
git clone https://github.com/SAM-DEV007/Image-Caption.git
cd Image-Caption
```

## Create a virtual environment
```bash
pip install virtualenv
python -m venv .venv
```

## Activate virtual environment
### Windows
```bash
./venv/Scripts/activate
```
### Linux/Mac
```bash
source venv/bin/activate
```

## Install Dependencies
(Refer to the [Requirements](#requirements) section for the updated model)
```bash
pip install -r requirements.txt
```

# Model
It uses `Mobile-Net V3 Large` feature extractor to extract the features from the image and Transformer-Decoder to generate the caption.

[Data_Preprocess](Model/Data_Preprocess.ipynb) file contains the code for the preprocessing of the dataset. **(The dataset and its caching space may go over 8GB)**

[Model_Train](Model/Model_Train.ipynb) file contains the code for the training of the model. 

Normal Model changeable parameters (included in the download link):
- `num_layers` = 5
- `units` = 512
- `num_heads` = 3
- `dropout_rate` = 0.4

Mobile Model changeable parameters (included in the repo):
- `num_layers` = 2
- `units` = 256
- `num_heads` = 2
- `dropout_rate` = 0.5

# Working
`caption.py` to initiate and use the model. [Images](Images) folder contains the images to be captioned (sample images are included) and the captioned images are in [Captioned](Images/Captioned) folder generated by the script.

# Web Integration
Refer to [here](https://github.com/SAM-DEV007/ML-Web-Integration/tree/main/image_caption) for the file structure and script changes made for Django.

The following steps can be followed to integrate the model in a web application:
1. Follow the [Installation](#installation) guide. The steps can be combined in a single bash script during server deployment.
2. Delete the `Images` folder as it is only required for user-based execution and not server execution.
3. Load the image received from the client in RGB format.
4. In `caption.py`, make sure `if __name__ == '__main__':` is changed to a function name so that it can be called repeatedly.\
   In the same code block, till the line `model = create_model(tokenizer, mobilenet, output_layer, weights_path)` is revelant. After this line, from the comment `Clears Captioned folder`, the code has no value for web developement.

Information on the main functions:
- `output = model.simple_gen(load_image(<image_object>))` is used to generate the caption of the image. `output` contains the caption.
- `img = add_caption(output, <image_object>)` can be used to combine the caption generated and the image in a single combined image. `output` is the caption generated from the model. `img` contains the combined image.

*NOTE: The function `load_image` and `add_caption` has to be modified in order to work with the image object. Currently, these functions take image path as the argument and opens the image instead of directly applying the operations.*
